# -*- coding: utf-8 -*-
"""Titanic Machine Learning from disaster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hfGZMaCufH4pnBtN40bFFI98BTLMh4BH
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # calculations
import pandas as pd #data analysis
import matplotlib.pyplot as plt
import seaborn as sns

# % matplotlib inline

# Read the Training 
train = pd.read_csv("/content/train.csv")
train.head(10)

train.shape

#Read Test Data
test = pd.read_csv("/content/test.csv")
test.head(20)

test.shape

data = train.append(test)

data.shape

passenger_id = test['PassengerId']

train.head()

train.set_index(['PassengerId'],inplace = True)
test.set_index(['PassengerId'],inplace = True)

train.head()

test.head()

train.isnull().sum()

test.isnull().sum()

train.dtypes

test.dtypes

import missingno as mn

mn.matrix(train)

mn.matrix(test)

from sklearn.preprocessing import Imputer

imp = Imputer(missing_values = 'NaN',strategy = 'median',axis = 1)

Age2 = imp.fit_transform(train['Age'].values.reshape(1,-1))
Age2

Age2 = Age2.T

Age2

train['Age2'] = Age2

train.head()

train.isnull().sum()

Age_test = imp.fit_transform(test['Age'].values.reshape(1,-1))

Age_test = Age_test.T

test['Age_test'] = Age_test

train.head()

test.head()

train.Embarked.value_counts()

test.Pclass.value_counts()

train.Embarked.fillna('S',inplace = True)

train.isnull().sum()

test.Fare.fillna(test.Fare.mean,inplace = True)

test.isnull().sum()

train.drop(['Age','Cabin'],axis=1,inplace=True)

test.drop(['Age','Cabin'],axis = 1,inplace = True)

test.isnull().sum()

train.isnull().sum()

train.head()

test.head()

train.Sex.value_counts()

train['Sex'] = train.Sex.apply(lambda x:0 if x == 'female' else 1)

train.head(10)

test.head(10)

test.head()

sns.boxplot(x = 'Survived', y = 'Fare',data = train)

train['Fare'] = train[train['Fare']<= 400]

train.describe()

train.groupby('Survived').mean()

train.groupby('Sex').mean()

train.corr()

plt.subplots(figsize = (15,8))
sns.heatmap(train.corr(),annot=True,cmap='PiYG')

sns.barplot(x = 'Sex',y = 'Survived',data = train )

sns.barplot(x = 'Pclass',y = 'Survived',data = train)

plt.subplots(figsize = (15,8))
sns.kdeplot(train.loc[(train['Survived']== 0),'Age2'],color = 'r',shade=True,Label = 'Not Survived')
sns.kdeplot(train.loc[(train['Survived']== 1),'Age2'],color = 'b',shade=True,Label = 'Survived')

train['family_size'] = train['SibSp']+train['Parch']+1
train.family_size.value_counts()

train.head()

def family_group(size):
  a = ''
  if(size <= 1):
    a = 'alone'
  elif(size <= 4):
    a = 'small'
  else:
    a = 'large'
  return a
train['family_group'] = train.family_size.map(family_group)
train.head()

def age_group(age):
  a = ''
  if (age <= 1):
    a = 'Infant'
  elif (age <=4):
    a = 'toddler'
  elif (age <= 14):
    a = 'Child'
  elif (age < 20):
    a = 'Teenager'
  elif (age <= 25):
    a = 'Young Adult'
  elif (age <= 40):
    a = 'Adult'
  elif (age <= 55):
    a = 'Middle Age'
  else:
    a = 'Old'
  return a
train['age_group'] = train.Age2.map(age_group)

train.head(10)

train['Fare_Per_Person'] = train['Fare']/train['family_size']

train.head(10)

def fare_group(fare):
  a = ''
  if (fare <= 4):
    a = 'Very Low'
  elif (fare <= 10):
    a = 'Low'
  elif (fare <= 20):
    a = 'Mid'
  elif (fare <= 45):
    a = 'High'
  else:
    a = 'Very High'
  return a
train['Fare_Group'] = train.Fare_Per_Person.map(fare_group)

train.head(10)

train = pd.get_dummies(train,columns=['Embarked','family_group','age_group','Fare_Group'],drop_first=True)

train.head(10)

train.shape

train.drop(['Ticket','Name','Fare','Age2','Fare_Per_Person','family_size'],axis=1,inplace = True)

train.shape

X = train.drop('Survived',1)
Y = train['Survived']

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedShuffleSplit, train_test_split
from sklearn.metrics import accuracy_score,log_loss
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis

Classifiers = [ 
               KNeighborsClassifier(3),
               svm.SVC(probability = True),
               DecisionTreeClassifier(),
               XGBClassifier(),
               RandomForestClassifier(),
               AdaBoostClassifier(),
               GradientBoostingClassifier(),
               GaussianNB(),
               LinearDiscriminantAnalysis(),
               QuadraticDiscriminantAnalysis(),
               LogisticRegression()
]

log_cols = ["Classifier","Accuracy"]
log = pd.DataFrame(columns=log_cols)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split,StratifiedShuffleSplit

SSplit = StratifiedShuffleSplit(test_size= 0.3,random_state= 7)
acc_dict = {}
for train_index,test_index in SSplit.split(X,Y):
    X_train,X_test = X.iloc[train_index],X.iloc[test_index]
    Y_train,Y_test = Y.iloc[train_index],Y.iloc[test_index]
    
    for clf in classifiers:
       name = clf.__class__.__name__

      clf.fit(X_train,Y_train)
      predict = clf.predict(X_test)
      acc = accuracy_score(Y_test,predict)
      if name in acc_dict:
        acc_dict[name]+=acc
      else:
          acc_dict[name] = acc

